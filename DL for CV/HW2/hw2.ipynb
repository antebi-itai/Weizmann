{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"hw2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"u3ku6Rrgxz-b","executionInfo":{"status":"ok","timestamp":1619430546055,"user_tz":-180,"elapsed":15039,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}},"outputId":"78721079-f6c4-42fd-ef20-c71862186d1d"},"source":["#@title ## Mount Your Google Drive\n","\n","#@markdown The next two cells are **magic** cells.\n","#@markdown They look like text cells, but they run code behind the scenes.\n","#@markdown You can run them by either clicking on the ‚ñ∂Ô∏è button (to the left of the cell), or by clicking on the cell and typing `Ctrl+Enter` (or `Shift+Enter`).\n","\n","#@markdown Please run this cell and follow the steps printed after running it. Specifically, it will print a URL you should enter, follow the instructions there and paste the code in the textbox below (and type `Enter`).\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"5iG-fCsoqvhu","executionInfo":{"status":"ok","timestamp":1619430546453,"user_tz":-180,"elapsed":15386,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}},"outputId":"e03fd282-f58c-46fc-e67c-ab043daa185c"},"source":["#@title ## Map Your Directory\n","import os\n","\n","def check_assignment(assignment_dir, files_list):\n","  files_in_dir = set(os.listdir(assignment_dir))\n","  for fname in files_list:\n","    if fname not in files_in_dir:\n","      raise FileNotFoundError(f'could not find file: {fname} in assignment_dir')\n","\n","assignment_dest = \"/content/hw2\"\n","assignment_dir = \"/content/gdrive/MyDrive/DL4CV/hw2\"  #@param{type:\"string\"}\n","assignment_files = ['hw2.ipynb', 'autograd.py', 'functional.py', 'nn.py', 'optim.py',\n","                    'models.py', 'models_torch.py', 'train.py', 'train_torch.py', 'utils.py',\n","                    'test_autograd.py', 'test_functional.py', 'test_nn.py', 'test_optim.py']\n","\n","# check Google Drive is mounted\n","if not os.path.isdir(\"/content/gdrive\"):\n","  raise FileNotFoundError(\"Your Google Drive isn't mounted. Please run the above cell.\")\n","\n","# check all files there\n","check_assignment(assignment_dir, assignment_files)\n","\n","# create symbolic link\n","!rm -f {assignment_dest}\n","!ln -s \"{assignment_dir}\" \"{assignment_dest}\"\n","print(f'Succesfully mapped (ln -s) \"{assignment_dest}\" -> \"{assignment_dir}\"')\n","\n","# cd to linked dir\n","%cd -q {assignment_dest}\n","print(f'Succesfully changed directory (cd) to \"{assignment_dest}\"')\n","#@markdown Set the path `assignment_dir` to the assignment directory in your Google Drive and run this cell.\n","\n","#@markdown If you are not sure what is the path, you can use the **Files (üìÅ)** menu (on the left side) to check the path."],"execution_count":2,"outputs":[{"output_type":"stream","text":["Succesfully mapped (ln -s) \"/content/hw2\" -> \"/content/gdrive/MyDrive/DL4CV/hw2\"\n","Succesfully changed directory (cd) to \"/content/hw2\"\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pIfpUXsh9qmW"},"source":["## Imports and `autoreload`-Magic\n","Please run the cell below (only once) to load and set the `autoreload` magic, which automatically reloads the import calls to the python files with your solutions. That means that you can edit the files (in the right-side window), save them (`Ctrl+S`) and just re-run the relevant cells -- the new code will kick in automatically.\n","\n","**Note:** You **MUST NOT** install any package. If you can't load something, you probably didn't follow the instructions (either didn't uploaded all the files, didn't mounted your Google driver or didn't mapped your directory).\n","\n","**Note:** The exercise works as is. If you add or modify imports to things, it may break thing in the notebook. You may do so **AT YOUR OWN RISK**. We will not assist with issues in notebook with modified imports.\n","\n","**Note:** Make sure you run **all the cells** up to the point. Some cells depends on previous cells (mainly imports). Furthermore, make sure to run the cell below (with the autoreload magic) before any cell below it."]},{"cell_type":"code","metadata":{"id":"x_Xg1LYZlnQ9","executionInfo":{"status":"ok","timestamp":1619430549791,"user_tz":-180,"elapsed":18723,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}}},"source":["import torch\n","\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4rwfbzEQnoXr"},"source":["# (A) Implement Components for Deep Neural Network From Scratch\n","\n","In This part you will implement a deep neural network from scratch, including the necessary building blocks. You will implement it in the following order:\n","\n","1. **Differentiable Functions:** a set of differentiable functions that are used as atomic building blocks.\n","2. **Autograd's backward:** the back-propagation `backward` method.\n","3. **Learnable Layers:** the Linear layer.\n","4. **Optimizer:** the SGD optimizer which will be used for training.\n"]},{"cell_type":"markdown","metadata":{"id":"u32VOx2k3Pb8"},"source":["## (A.1) Differentiable Functions\n","\n","In this section you will implement a set of differentiable functions from scratch. For each function, you will implement the forward and backward methods. After the description of the method, there is a testing cell which we will test the correctness of your code.\n","\n","The skeletons of the differential functions to implement are in the `functional.py` file. Open this file by clicking on this link: `/content/hw2/functional.py`. Alternatively, you can go the left menu, click on **Files (üìÅ)**, go to the directory `hw2` (or `content/hw2`) and double-click on `functional.py` to open it. The tests can be found in `test_functional.py` (link: `/content/hw2/test_functional.py`).\n","\n","In each step you should fill the blanks (between `# BEGIN SOLUTION` and `# END SOLUTION`) in the relevant methods. DO NOT change any other code segments. You are provided with a cell to run the tests, and with a cell to debug your code (with the relevant imports). As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n","\n","### `ctx`\n","In the \"from scratch\" implementation, you should use a `ctx` (context) variable. This variable is a simplified version of the computation graph, and is needed for the back propagation algorithm.\n","\n","Specifically, `ctx` is just a list (or stack) of \"backward calls\", where each \"backward call\" is a pair (list/tuple) of two objects:\n","\n","1. **`backward_fn`:** The backward function. A reference to the backward function to be called in the backward pass.\n","2. **`args`:** A list (or tuple) of arguments to be passed to `backward_fn`. This list usually consists of the inputs and the outputs of the forward function. Sometimes additional arguments are passed as well. It's important to pass the actual inputs and outputs (same pointer), otherwise it would break the chain of gradients propagation.\n","\n","The \"backward calls\" in `ctx` should be ordered in according to the time of addition. That is, a backward call that was added later should have an higher index in the list `ctx`. If `ctx` is `None`, it means that gradients (i.e. backward calls) should not be tracked.\n","\n","You will use `ctx` in the backward pass in section (A.2). You can read it now to get a little context (pun intended).\n","\n","**Note:** You are given an example of the forward and backward implementation of `mean`. You should read and understand how new backward calls are appended to `ctx`, and use this pattern in your solutions.\n","\n","**Note:** When new tensors are created (using `zeros`, `ones`, `rand`, etc.), it's important to make sure they are on the same device (and has the correct `dtype`) as tensors they would be used together with (compared to, multiplied by, etc.). You may find the functions `torch.X_like` and `Tensor.new_X` handy."]},{"cell_type":"markdown","metadata":{"id":"KxRhfmjXODEo"},"source":["### (A.1.1) Implement the Linear Function\n","\n","Here you will implement a differentiable `linear` function. This includes the forward `linear` function and the backward `linear_backward` function.\n","\n","#### `linear`\n","The `linear` function receives three arguments (in addition to the autograd context `ctx`):\n","\n","  * `x`: The batched input. Has shape `(batch_size, in_dim)`.\n","  * `w`: The weight matrix. Has shape `(out_dim, in_dim)`.\n","  * `b`: The bias term. Has shape `(out_dim,)`.\n","\n","It computes the (batched version of the) function: $$ \\mathbf{y} = W \\mathbf{x} + \\mathbf{b} $$\n","The output `y` should have shape `(batch_size, out_dim)`.\n","\n","#### `linear_backward`\n","The `linear_backward` function receives four arguments:\n","\n","  * `y`: The batched output. Has shape `(batch_size, out_dim)`.\n","  * `x`: The batched input. Has shape `(batch_size, in_dim)`.\n","  * `w`: The weight matrix. Has shape `(out_dim, in_dim)`.\n","  * `b`: The bias term. Has shape `(out_dim,)`.\n","\n","It computes the gradients of `x`, `w` and `b` w.r.t the loss, given the gradient of `y` (in `y.grad`) w.r.t the loss, and accumulates these gradients in `x.grad`, `w.grad` and `b.grad`, respectively.\n","\n","---\n","You should test your solution by running the following cell. You can debug your solution in the cell below it."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OkrZhq0U31I0","executionInfo":{"status":"ok","timestamp":1619430552642,"user_tz":-180,"elapsed":21565,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}},"outputId":"181df846-5a52-4b43-c253-09317ee24f06"},"source":["!python -m unittest test_functional.TestLinear"],"execution_count":4,"outputs":[{"output_type":"stream","text":["......\n","----------------------------------------------------------------------\n","Ran 6 tests in 0.309s\n","\n","OK\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UAqdVJl9uxrI","executionInfo":{"status":"ok","timestamp":1619430552645,"user_tz":-180,"elapsed":21566,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}}},"source":["# Playground for debugging linear\n","from functional import linear, linear_backward"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w3vGFltjkDSm"},"source":["### (A.1.2) Implement the ReLU Activation\n","\n","Here you will implement a differentiable `relu` activation. This includes the forward `relu` function and the backward `relu_backward` function.\n","\n","#### `relu`\n","The `relu` function receives one argument (in addition to the autograd context `ctx`):\n","\n","  * `x`: The input. Has an arbitrary shape.\n","\n","It computes the (element-wise) function:\n","$$ y = \\max(x, 0) $$\n","The output `y` should have the same shape as `x`.\n","\n","#### `relu_backward`\n","The `relu_backward` function receives two arguments:\n","\n","  * `y`: The output. Has the same shape as `x`.\n","  * `x`: The input. Has an arbitrary shape.\n","\n","It computes the gradients of `x` w.r.t the loss, given the gradient of `y` (in `y.grad`) w.r.t the loss, and accumulates this gradient in `x.grad`.\n","\n","---\n","You should test your solution by running the following cell. You can debug your solution in the cell below it."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lA-NX0Y-j8yF","executionInfo":{"status":"ok","timestamp":1619430553771,"user_tz":-180,"elapsed":22684,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}},"outputId":"80a95ba3-8a66-4fe1-f05e-9ceef34eede4"},"source":["!python -m unittest test_functional.TestReLU"],"execution_count":6,"outputs":[{"output_type":"stream","text":[".......\n","----------------------------------------------------------------------\n","Ran 7 tests in 0.009s\n","\n","OK\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m_79D4JGj888","executionInfo":{"status":"ok","timestamp":1619430553773,"user_tz":-180,"elapsed":22683,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}}},"source":["# Playground for debugging relu\n","from functional import relu, relu_backward"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QKtM0ZFakD3q"},"source":["### (A.1.3) Implement the Softmax Activation\n","\n","Here you will implement a differentiable `softmax` activation. This includes the forward `softmax` function and the backward `softmax_backward` function.\n","\n","**Note:** Similarly to homework assignment #1, your solution should be numerically stable.\n","\n","#### `softmax`\n","The `softmax` function receives one argument (in addition to the autograd context `ctx`):\n","\n","  * `x`: The batched input. Has shape `(batch_size, num_classes)`.\n","\n","It computes the (batched version of the) function: $$ \\mathbf{y}_i = \\frac{e^{\\mathbf{x}_i}}{\\sum_j{e^{\\mathbf{x}_j}}} $$\n","The output `y` should have the shape `(batch_size, num_classes)`. Each row in `y` should be a probability distribution over the classes.\n","\n","\n","#### `softmax_backward`\n","The `softmax_backward` function receives two arguments:\n","\n","  * `y`: The batched output. Has shape `(batch_size, num_classes)`.\n","  * `x`: The batched input. Has shape `(batch_size, num_classes)`.\n","\n","It computes the gradients of `x` w.r.t the loss, given the gradient of `y` (in `y.grad`) w.r.t the loss, and accumulates this gradient in `x.grad`.\n","\n","---\n","You should test your solution by running the following cell. You can debug your solution in the cell below it."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AqSS59Pej9FY","executionInfo":{"status":"ok","timestamp":1619430554435,"user_tz":-180,"elapsed":23337,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}},"outputId":"e7783d2b-b3b8-468d-bd90-cfdde6296431"},"source":["!python -m unittest test_functional.TestSoftmax"],"execution_count":8,"outputs":[{"output_type":"stream","text":[".......\n","----------------------------------------------------------------------\n","Ran 7 tests in 0.026s\n","\n","OK\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jHN0ZW5Xj9Lv","executionInfo":{"status":"ok","timestamp":1619430554437,"user_tz":-180,"elapsed":23337,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}}},"source":["# Playground for debugging softmax\n","from functional import softmax, softmax_backward"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zL1_-cE5kEiG"},"source":["### (A.1.4) Implement the Cross-Entropy Loss\n","\n","Here you will implement a differentiable `cross_entropy` activation. This includes the forward `cross_entropy` function and the backward `cross_entropy_backward` function.\n","\n","**Note:** Similarly to homework assignment #1, your solution should be numerically stable.\n","\n","**Note:** The signature of this function differs from PyTorch's `F.cross_entropy`. The function you should implement doesn't \"reduce\" (i.e. averages over) the batch (similarly to `F.cross_entropy(..., reduction='none')`). Furthermore, while `F.cross_entropy` receives the predictions **before** `softmax`, the function you should implement receives the predictions **after** `softmax`. We provide you the `cross_entropy_loss` which uses your implementation of `softmax` and `cross_entropy`, and has the same API as `F.cross_entropy`.\n","\n","#### `cross_entropy`\n","The `cross_entropy` function receives two arguments (in addition to the autograd context `ctx`):\n","\n","  * `pred`: The predicted _probabilities_. Has shape `(batch_size, num_classes)`. Each row is a probability distribution (non-negative values; sums to 1).\n","  * `target`: The batched correct labels. Has type of `torch.long` (integer values), and has shape `(batch_size,)`. Its values are between `0` and `num_classes - 1` (inclusive).\n","\n","It computes the (batched version of the) function:\n","$$ \\text{CE}(\\hat{\\mathbf{y}}, \\ell)_i = -\\log(\\hat{\\mathbf{y}}_i) \\cdot \\delta_{i,\\ell} $$\n","Where $\\hat{\\mathbf{y}}$ (also called `pred` or `y_hat`) is the predicted probability measure over the classes and $\\ell$ (also called `target` or `y`) is the target class label.\n","\n","The output `loss` should have the shape `(batch_size,)`. Each row in `loss` should be the cross-entropy loss of that entry in the batch.\n","\n","#### `cross_entropy_backward`\n","The `cross_entropy_backward` function receives three arguments:\n","\n","  * `loss`: The batched loss. Has shape `(batch_size,)`.\n","  * `pred`: The batched predicted _probabilities_. Has shape `(batch_size, num_classes)`. Each row is a probability distribution (non-negative values; sums to 1).\n","  * `target`: The batched correct labels. Has type of `torch.long` (integer values), and has shape `(batch_size,)`. Its values are between `0` and `num_classes - 1` (inclusive).\n","\n","It computes the gradients of `pred` w.r.t the (final scalar) loss, given the gradient of (batched) `loss` (in `loss.grad`) w.r.t the loss, and accumulates this gradient in `pred.grad`.\n","\n","#### `cross_entropy_loss`\n","This function is provided for your use. It calls `softmax` to compute the probability distribution over the labels, then `cross_entropy` to computed the batched loss, and later `mean` to reduce it into a scalar loss (that can be used as the origin of gradients; see next part). You should NOT modify this method, and may use it later on.\n","\n","**Note:** Please see how three differentiable functions (`softmax`, `cross_entropy` and `mean`) are chained to create a new differentiable function, without explicitly implementing its backward pass. You will chain differentiable functions to create a model in section (B).\n","\n","---\n","You should test your solution by running the following cell. You can debug your solution in the cell below it.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jW24PjRj9RE","executionInfo":{"status":"ok","timestamp":1619430555571,"user_tz":-180,"elapsed":24463,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}},"outputId":"d994428a-26ac-45e1-98a3-e4415432c1ef"},"source":["!python -m unittest test_functional.TestCrossEntropy"],"execution_count":10,"outputs":[{"output_type":"stream","text":[".......\n","----------------------------------------------------------------------\n","Ran 7 tests in 0.036s\n","\n","OK\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S6au2xEbj9Vu","executionInfo":{"status":"ok","timestamp":1619430555572,"user_tz":-180,"elapsed":24462,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}}},"source":["# Playground for debugging cross_entropy\n","from functional import cross_entropy, cross_entropy_backward\n","from functional import cross_entropy_loss"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7e_g4fDakFoQ"},"source":["## (A.2) Autograd\n","\n","In this section you will implement a general `backward` method from scratch. This method stands at the core of back-propagation and autograd differentiation.\n","\n","This method receives two arguments:\n","\n","* `loss`: The loss tensor. This tensor must be a scalar (Has shape `()`). The loss the other tensors will be computed w.r.t this `loss`.\n","* `ctx`: The autograd context. A list of backward calls. These backward calls should be evaluated to back-propagate the gradient from `loss` to the tensors used in the computation of `loss`.\n","\n","This method has two main steps:\n","\n","* Setting the gradient of `loss` (to what?).\n","* Propagating the gradients backward using the computation history in `ctx` (how?).\n","\n","The skeleton of the `backward` method is in the `autograd.py` file (link: `/content/hw2/autograd.py`). The tests can be found in `test_autograd.py` (link: `/content/hw2/test_autograd.py`). You should fill the blanks between `# BEGIN SOLUTION` and `# END SOLUTION`. DO NOT change any other code segments. You can use the provided `create_grad_if_necessary` which makes sure that tensors that need gradients have one (if not, it creates a `.grad` attribute in the tensor's shape filled with zeros). As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bTb1J0bFj9ag","executionInfo":{"status":"ok","timestamp":1619430557674,"user_tz":-180,"elapsed":26556,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}},"outputId":"5bc90a1e-6845-43ec-9c9f-77a37e1ef51f"},"source":["!python -m unittest test_autograd.TestBackward"],"execution_count":12,"outputs":[{"output_type":"stream","text":["...\n","----------------------------------------------------------------------\n","Ran 3 tests in 0.008s\n","\n","OK\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"veWDHxTTj9fl","executionInfo":{"status":"ok","timestamp":1619430557675,"user_tz":-180,"elapsed":26550,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}},"outputId":"bf92d4b1-574c-49a5-e5e2-6269bf6ca5f0"},"source":["# Playground for debugging backward\n","from autograd import backward\n","from functional import cross_entropy_loss\n","\n","# You CAN modify the content of the cell below. It is just an example.\n","ctx = []\n","x = torch.randn(4, 5)\n","l = torch.randint(5, size=(4,), dtype=torch.long)\n","y = softmax(x, ctx=ctx)\n","loss = cross_entropy_loss(y, l, ctx=ctx)\n","\n","print('before backward')\n","print('loss.grad:', loss.grad)\n","print('y.grad:', y.grad)\n","print('x.grad:', x.grad)\n","\n","backward(loss, ctx)\n","\n","print('\\n\\nafter backward')\n","print('loss.grad:', loss.grad)\n","print('y.grad:', y.grad)\n","print('x.grad:', x.grad)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["before backward\n","loss.grad: None\n","y.grad: None\n","x.grad: None\n","\n","\n","after backward\n","loss.grad: tensor(1.)\n","y.grad: tensor([[ 0.0420, -0.1991,  0.0733,  0.0425,  0.0414],\n","        [ 0.0487,  0.0736, -0.2055,  0.0418,  0.0415],\n","        [ 0.0490,  0.0415,  0.0507,  0.0418, -0.1830],\n","        [ 0.0444,  0.0548,  0.0629,  0.0445, -0.2067]])\n","x.grad: tensor([[ 0.0020, -0.0490,  0.0430,  0.0026,  0.0014],\n","        [ 0.0026,  0.0230, -0.0261,  0.0003,  0.0002],\n","        [ 0.0231,  0.0033,  0.0275,  0.0040, -0.0579],\n","        [ 0.0005,  0.0048,  0.0106,  0.0005, -0.0165]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9F30SfhnmkWP"},"source":["## (A.3) Learnable Layers\n","\n","In this section you will implement a learnable Linear layer. The implementation is similar to vanilla PyTorch.\n","\n","The skeleton of the learnable Linear layer to implement is in the `nn.py` file (link: `/content/hw2/nn.py`). The tests can be found in `test_nn.py` (link: `/content/hw2/test_nn.py`).\n","\n","Learnable layers (and networks) inherits from the provided class `Module` (which is similar to PyTorch's `nn.Module`). This abstract class implements some utility methods (some are not used in this assignment). Please read the list of `Module`'s methods and attributes in its documentation (link: `/content/hw2/nn.py`).\n","\n","In the `nn.py` file, you should fill the blanks (between `# BEGIN SOLUTION` and `# END SOLUTION`) in the relevant methods. DO NOT change any other code segments. You are provided with a cell to run the tests, and with a cell to debug your code (with the relevant imports). As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n","\n","**Note:** To see how \"atomic\" differentiable functions are composed into a complex differentiable function, please look at the provided `cross_entropy_loss` in `/content/hw2/functional.py`.\n","\n","**Note:** Since this part doesn't use PyTorch's built-in autograd mechanism, please do not use tensors' `requires_grad` (this will result in errors/warnings).\n","Furthermore, do not use `nn.Parameter` in _from scratch_ layers."]},{"cell_type":"markdown","metadata":{"id":"6MkxNHgUnAoa"},"source":["### (A.3.1) Implement the Linear Layer\n","\n","So far you have implemented *stateless* differentiable functions, and the autograd mechanism. In this section, you will implement a *stateful* layer, with learnable parameters. That is the `Linear` layer.\n","\n","The parameters of the `Linear` layer are the weight matrix `weight` and the bias term `bias`. In your layer, you should:\n","\n","1. **Create parameter tensors:** create tensors for the parameters in the correct shape. The parameters should be attributes of the layer, i.e. set as `self.<param> = <tensor>`. This is done in `Linear.__init__`.\n","2. **Register them as parameters:** add their names to `self._parameters`. This will be used by the provided `Module.parameters()` (to list module's parameters) and `Module.to()` (to trasfer module's parameters to a device) methods. This is done in `Linear.__init__`.\n","3. **Initialize the parameters:** initialization of the layer parameters has significant influence on the local minimum the network reaches during training. This is done in `Linear.init_parameters()`. You should call this method from `Linear.__init__`, so newly created linear layers are initialized.\n","4. **Implement a forward method:** use the existing differentiable function from part A, and implement the `Linear.forward()` method."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ARMFodrXnBfW","executionInfo":{"status":"ok","timestamp":1619430559709,"user_tz":-180,"elapsed":28575,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}},"outputId":"83abf06d-5024-4b60-e8c8-572424860de3"},"source":["!python -m unittest test_nn.TestLinear"],"execution_count":14,"outputs":[{"output_type":"stream","text":["....\n","----------------------------------------------------------------------\n","Ran 4 tests in 0.011s\n","\n","OK\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wENev5CtcjnT","executionInfo":{"status":"ok","timestamp":1619430559710,"user_tz":-180,"elapsed":28574,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}}},"source":["# Playground for debugging Linear\n","from nn import Linear"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JTKP7Ovas-_M"},"source":["## (A.4) Optimizer\n","\n","In this section you will implement an optimizer. The optimizer updates the parameters based on the gradients they had accumulated. To do so it should have three main functions:\n","\n","1. `__init__`: Receives the list of parameters (weights) to update their values and save them. May receive additional arguments, such as learning-rate, etc.\n","2. `step`: Updates the parameters values based on the value of their gradients. Doesn't receive any argument.\n","3. `zero_grad`: Zeros the gradients of the tracked parameters. This is necessary since gradients are accumulated in each backward pass, and we don't want to mix between batches. Doesn't receive any argument.\n","\n","The skeleton of the optimizer is in the `optim.py` file (link: `/content/hw2/optim.py`). The tests can be found in `test_optim.py` (link: `/content/hw2/test_optim.py`). You should fill the blanks between `# BEGIN SOLUTION` and `# END SOLUTION`. DO NOT change any other code segments. As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n"]},{"cell_type":"markdown","metadata":{"id":"PK5kaf5kmoxM"},"source":["### (A.4.1) SGD Optimizer\n","In this part, you'll implement an SGD optimizer. This optimizer has a simple update rule, which is:\n","$$\\mathbf{x}_{n+1} = \\mathbf{x}_{n} - \\delta \\cdot \\mathbf{g}_{n} $$\n","Where $\\mathbf{x}_{n}$ is the parameter at step $n$, $\\mathbf{g}_{n}$ is its gradient at step $n$, and $\\delta$ is the learning rate (also called `lr`).\n","\n","You should implement the `__init__`, `step` and `zero_grad` methods of `SGD` optimizer in `optim.py`.\n","\n","**Note:** Parameters (tensors) should be updated **in-place** (i.e. with the `-=` operator) in `step`.\n","\n","**Note:** A gradient (`param.grad`) which is set to `None` is also considered as zero."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iR4bRY16aW1Y","executionInfo":{"status":"ok","timestamp":1619430561543,"user_tz":-180,"elapsed":30400,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}},"outputId":"9f982e54-8e1a-49ff-b8d9-862652e05165"},"source":["!python -m unittest test_optim.TestSGD"],"execution_count":16,"outputs":[{"output_type":"stream","text":["..\n","----------------------------------------------------------------------\n","Ran 2 tests in 0.004s\n","\n","OK\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dZlZwEqGj9_2","executionInfo":{"status":"ok","timestamp":1619430561545,"user_tz":-180,"elapsed":30400,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}}},"source":["# Playground for debugging SGD\n","from optim import SGD"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XStdhdlPcSBd"},"source":["# Setup Before Training\n","\n","In this part you will need to use GPU (this will have a significant impact on the training speed). To get a GPU in Google Colab, please go to the top menu and to: **Runtime ‚ûî Change runtime type**. Then, select **GPU** as **Hardware accelerator**.\n","\n","Please run the cell below to set your pytorch device (either GPU or CPU), to load the dataset and to create data loaders.\n","\n"]},{"cell_type":"code","metadata":{"id":"cVRcw-dlgFLb","executionInfo":{"status":"ok","timestamp":1619430565430,"user_tz":-180,"elapsed":34283,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}}},"source":["from utils import load_mnist\n","\n","# Set the device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","pin_memory = device.type == 'cuda'\n","\n","# Load the training and test sets\n","train_data = load_mnist(mode='train')\n","test_data = load_mnist(mode='test')\n","\n","# Create dataloaders for training and test sets\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, pin_memory=pin_memory)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, pin_memory=pin_memory)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GiJaEoARnAOn"},"source":["# (B) Define and Train Neural Networks From Scratch\n","\n","\n","In this part, you will define and train neural networks from scratch. You will use your differentiable functions from section (A).\n","\n","The skeletons for this assignment can be found in the `models.py` (link: `/content/hw2/models.py`) and `train.py` (link: `/content/hw2/train.py`) files. You should fill the blanks between `# BEGIN SOLUTION` and `# END SOLUTION`. As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n","\n","Please run the cell below to import the relevant objects in order to train the models."]},{"cell_type":"code","metadata":{"id":"PzTMHw9yKxN6","executionInfo":{"status":"ok","timestamp":1619430565432,"user_tz":-180,"elapsed":34283,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}}},"source":["from functional import cross_entropy_loss as cross_entropy_scratch\n","from models import SoftmaxClassifier as SoftmaxClassifierScratch\n","from models import MLP as MLPScratch\n","from optim import SGD as SGDScratch\n","from train import train_loop as train_loop_scratch"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EW255-xmnA6O"},"source":["## (B.1) Implement and Train a SoftmaxClassifier\n","\n","Here you will implement the `SoftmaxClassifier` (imported here as `SoftmaxClassifierScratch`). You have already implemented the `SoftmaxClassifier` in Homework 1, but now it will be implemented with autograd and modular differentiable functions.\n","\n","Your solution should have the following parts:\n","\n","1. Create a model.\n","2. (Optional) Transfer the model to `device`.\n","3. Create an optimizer. (this should be done when the model is in its final device. It will not work otherwise).\n","4. Set other hyper-parameters (loss function, number of epochs, etc.).\n","5. Train the model.\n","\n","**Note:** As opposed to its name, `SoftmaxClassifier` should not perform softmax. That's because softmax part of the cross-entropy loss (in PyTorch and in the _from scratch_ section).\n"]},{"cell_type":"code","metadata":{"id":"EQlsopQc3dF5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619430679063,"user_tz":-180,"elapsed":147906,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}},"outputId":"d9022618-0936-4c9f-fcf9-7f8b8e916bf0"},"source":["# BEGIN SOLUTION\n","IMAGE_SIZE = 28*28\n","NUM_CLASSES = 10\n","\n","lr = 1e-2\n","\n","# Define your model\n","model = SoftmaxClassifierScratch(in_dim=IMAGE_SIZE, num_classes=NUM_CLASSES)\n","\n","# Transfer it to device\n","model = model.to(device)\n","\n","# Set an optimizer\n","optimizer = SGDScratch(parameters=model.parameters(), lr=lr)\n","\n","# Set a criterion (loss function)\n","criterion = cross_entropy_scratch\n","\n","# Set the number of epochs\n","epochs = 10\n","\n","# Train your model\n","train_loop_scratch(model=model,\n","                   criterion=criterion,\n","                   optimizer=optimizer,\n","                   train_loader=train_loader,\n","                   test_loader=test_loader,\n","                   device=device,\n","                   epochs=epochs)\n","# END SOLUTION"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Train   Epoch: 001 / 010   Loss:  0.4728   Accuracy: 0.871\n"," Test   Epoch: 001 / 010   Loss:   0.331   Accuracy: 0.908\n","Train   Epoch: 002 / 010   Loss:  0.3331   Accuracy: 0.905\n"," Test   Epoch: 002 / 010   Loss:  0.3064   Accuracy: 0.915\n","Train   Epoch: 003 / 010   Loss:  0.3112   Accuracy: 0.912\n"," Test   Epoch: 003 / 010   Loss:  0.2944   Accuracy: 0.917\n","Train   Epoch: 004 / 010   Loss:  0.3001   Accuracy: 0.915\n"," Test   Epoch: 004 / 010   Loss:  0.2905   Accuracy: 0.918\n","Train   Epoch: 005 / 010   Loss:  0.2928   Accuracy: 0.916\n"," Test   Epoch: 005 / 010   Loss:  0.2845   Accuracy: 0.918\n","Train   Epoch: 006 / 010   Loss:  0.2873   Accuracy: 0.919\n"," Test   Epoch: 006 / 010   Loss:  0.2803   Accuracy: 0.920\n","Train   Epoch: 007 / 010   Loss:  0.2832   Accuracy: 0.920\n"," Test   Epoch: 007 / 010   Loss:  0.2819   Accuracy: 0.919\n","Train   Epoch: 008 / 010   Loss:  0.2801   Accuracy: 0.921\n"," Test   Epoch: 008 / 010   Loss:  0.2798   Accuracy: 0.920\n","Train   Epoch: 009 / 010   Loss:  0.2774   Accuracy: 0.922\n"," Test   Epoch: 009 / 010   Loss:   0.277   Accuracy: 0.920\n","Train   Epoch: 010 / 010   Loss:  0.2751   Accuracy: 0.923\n"," Test   Epoch: 010 / 010   Loss:  0.2772   Accuracy: 0.921\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SNtYgm9vnGtR"},"source":["## (B.2) Implement and Train a Deep Neural Network\n","\n","Here you will implement a multi-layer perceptron (`MLP`) model (imported here as `MLPScratch`). You are allowed to modify the signiture of `MLP.__init__` and add additional arguments to your choice. Your network must have more than a single linear layer.\n","\n","Your solution should have the same parts as in (B.1)."]},{"cell_type":"code","metadata":{"id":"vVU5EM6CSgHL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619430787143,"user_tz":-180,"elapsed":255975,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}},"outputId":"0d4c75e4-cc5e-44da-d5b7-a0b38bd32497"},"source":["# BEGIN SOLUTION\n","IMAGE_SIZE = 28*28\n","NUM_CLASSES = 10\n","\n","lr = 1e-1\n","hidden_size = 256\n","\n","# Define your model\n","model = MLPScratch(in_dim=IMAGE_SIZE, num_classes=NUM_CLASSES, hidden_size=hidden_size)\n","\n","# Transfer it to device\n","model = model.to(device)\n","\n","# Set an optimizer\n","optimizer = SGDScratch(parameters=model.parameters(), lr=lr)\n","\n","# Set a criterion (loss function)\n","criterion = cross_entropy_scratch\n","\n","# Set the number of epochs\n","epochs = 10\n","\n","# Train your model\n","train_loop_scratch(model=model,\n","                   criterion=criterion,\n","                   optimizer=optimizer,\n","                   train_loader=train_loader,\n","                   test_loader=test_loader,\n","                   device=device,\n","                   epochs=epochs)\n","# END SOLUTION"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Train   Epoch: 001 / 010   Loss:  0.2801   Accuracy: 0.916\n"," Test   Epoch: 001 / 010   Loss:  0.1528   Accuracy: 0.950\n","Train   Epoch: 002 / 010   Loss:  0.1016   Accuracy: 0.968\n"," Test   Epoch: 002 / 010   Loss:  0.1094   Accuracy: 0.965\n","Train   Epoch: 003 / 010   Loss: 0.06624   Accuracy: 0.980\n"," Test   Epoch: 003 / 010   Loss: 0.07922   Accuracy: 0.973\n","Train   Epoch: 004 / 010   Loss: 0.04675   Accuracy: 0.985\n"," Test   Epoch: 004 / 010   Loss: 0.06431   Accuracy: 0.979\n","Train   Epoch: 005 / 010   Loss: 0.03444   Accuracy: 0.989\n"," Test   Epoch: 005 / 010   Loss: 0.07117   Accuracy: 0.977\n","Train   Epoch: 006 / 010   Loss: 0.02533   Accuracy: 0.992\n"," Test   Epoch: 006 / 010   Loss: 0.06336   Accuracy: 0.982\n","Train   Epoch: 007 / 010   Loss: 0.01804   Accuracy: 0.995\n"," Test   Epoch: 007 / 010   Loss: 0.07014   Accuracy: 0.979\n","Train   Epoch: 008 / 010   Loss: 0.01326   Accuracy: 0.996\n"," Test   Epoch: 008 / 010   Loss: 0.06429   Accuracy: 0.981\n","Train   Epoch: 009 / 010   Loss: 0.008497   Accuracy: 0.998\n"," Test   Epoch: 009 / 010   Loss: 0.06923   Accuracy: 0.981\n","Train   Epoch: 010 / 010   Loss: 0.005481   Accuracy: 0.999\n"," Test   Epoch: 010 / 010   Loss: 0.06725   Accuracy: 0.983\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2PSeMqMC3fBc"},"source":["# (C) Define and Train PyTorch Neural Networks\n","\n","In this part, you will define and train neural networks using PyTorch's built-in autograd mechanism. You MAY NOT use your differentiable functions from section (A). The solution to this part is very similar to the solution of part (B), with some syntax changes.\n","\n","The skeletons for this assignment can be found in the `models_torch.py` (link: `/content/hw2/models_torch.py`) and `train_torch.py` (link: `/content/hw2/train_torch.py`) files. You should fill the blanks between `# BEGIN SOLUTION` and `# END SOLUTION`. As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n","\n","Please run the cell below to import the relevant objects in order to train the models.\n","\n","**Note:** some methods are imported with different names in this notebook to distinguish them from the _From Scratch_ part. This is not a best practice, and used solely as a way to avoid ambiguities in this assignment."]},{"cell_type":"code","metadata":{"id":"1eMq2E6F3kwR","executionInfo":{"status":"ok","timestamp":1619430787486,"user_tz":-180,"elapsed":256316,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}}},"source":["# NOTE: `cross_entropy_torch` is different from `cross_entropy_scratch`!\n","# cross_entropy_torch(pred, target) == cross_entropy_scratch(softmax(pred), target)\n","from torch.nn.functional import cross_entropy as cross_entropy_torch\n","from models_torch import SoftmaxClassifier as SoftmaxClassifierTorch\n","from models_torch import MLP as MLPTorch\n","from torch.optim import SGD as SGDTorch\n","from train_torch import train_loop as train_loop_torch\n","from utils import load_mnist"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fFI-VKr73bcE"},"source":["## (C.1) Implement and Train a Softmax Classifier\n","\n","Here you will implement the `SoftmaxClassifier` class (imported as `SoftmaxClassifierTorch`).\n","\n","Your solution should have the same parts as in (B.1)."]},{"cell_type":"code","metadata":{"id":"e0U2HGz13b0W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619430888237,"user_tz":-180,"elapsed":357059,"user":{"displayName":"Itai Antebi","photoUrl":"","userId":"07745069812465438779"}},"outputId":"80b4da3b-87ee-4825-847c-b4386f730e38"},"source":["# BEGIN SOLUTION\n","IMAGE_SIZE = 28*28\n","NUM_CLASSES = 10\n","\n","lr = 1e-1\n","\n","# Define your model\n","model = SoftmaxClassifierTorch(in_dim=IMAGE_SIZE, num_classes=NUM_CLASSES)\n","\n","# Transfer it to device\n","model = model.to(device)\n","\n","# Set an optimizer\n","optimizer = SGDTorch(model.parameters(), lr=lr)\n","\n","# Set a criterion (loss function)\n","criterion = cross_entropy_torch\n","\n","# Set the number of epochs\n","epochs = 10\n","\n","# Train your model\n","train_loop_torch(model=model,\n","                 criterion=criterion,\n","                 optimizer=optimizer,\n","                 train_loader=train_loader,\n","                 test_loader=test_loader,\n","                 device=device,\n","                 epochs=epochs)\n","# END SOLUTION"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Train   Epoch: 001 / 010   Loss:  0.3716   Accuracy: 0.891\n"," Test   Epoch: 001 / 010   Loss:    0.33   Accuracy: 0.903\n","Train   Epoch: 002 / 010   Loss:  0.3138   Accuracy: 0.910\n"," Test   Epoch: 002 / 010   Loss:  0.3322   Accuracy: 0.905\n","Train   Epoch: 003 / 010   Loss:  0.3023   Accuracy: 0.915\n"," Test   Epoch: 003 / 010   Loss:  0.3261   Accuracy: 0.911\n","Train   Epoch: 004 / 010   Loss:  0.2984   Accuracy: 0.915\n"," Test   Epoch: 004 / 010   Loss:   0.293   Accuracy: 0.917\n","Train   Epoch: 005 / 010   Loss:  0.2921   Accuracy: 0.917\n"," Test   Epoch: 005 / 010   Loss:  0.2893   Accuracy: 0.921\n","Train   Epoch: 006 / 010   Loss:  0.2894   Accuracy: 0.918\n"," Test   Epoch: 006 / 010   Loss:  0.2921   Accuracy: 0.923\n","Train   Epoch: 007 / 010   Loss:  0.2864   Accuracy: 0.920\n"," Test   Epoch: 007 / 010   Loss:  0.2982   Accuracy: 0.919\n","Train   Epoch: 008 / 010   Loss:  0.2846   Accuracy: 0.921\n"," Test   Epoch: 008 / 010   Loss:  0.3185   Accuracy: 0.911\n","Train   Epoch: 009 / 010   Loss:  0.2836   Accuracy: 0.922\n"," Test   Epoch: 009 / 010   Loss:  0.3101   Accuracy: 0.916\n","Train   Epoch: 010 / 010   Loss:  0.2811   Accuracy: 0.921\n"," Test   Epoch: 010 / 010   Loss:  0.3378   Accuracy: 0.906\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Gc3NLJiwXsuH"},"source":["## (C.2) Implement and Train a Deep Neural Network\n","\n","Here you will implement the `MLP` class (imported as `MLPTorch`).\n","\n","Your solution should have the same parts as in (B.2)."]},{"cell_type":"code","metadata":{"id":"JbXH5Dt4LVHu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7c430271-760e-4d9d-e1aa-e206f0805093"},"source":["# BEGIN SOLUTION\n","IMAGE_SIZE = 28*28\n","NUM_CLASSES = 10\n","\n","lr = 1e-1\n","hidden_size = 256\n","\n","# Define your model\n","model = MLPTorch(in_dim=IMAGE_SIZE, num_classes=NUM_CLASSES, hidden_size=hidden_size)\n","\n","# Transfer it to device\n","model = model.to(device)\n","\n","# Set an optimizer\n","optimizer = SGDTorch(model.parameters(), lr=lr)\n","\n","# Set a criterion (loss function)\n","criterion = cross_entropy_torch\n","\n","# Set the number of epochs\n","epochs = 10\n","\n","# Train your model\n","train_loop_torch(model=model,\n","                 criterion=criterion,\n","                 optimizer=optimizer,\n","                 train_loader=train_loader,\n","                 test_loader=test_loader,\n","                 device=device,\n","                 epochs=epochs)\n","# END SOLUTION"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train   Epoch: 001 / 010   Loss:  0.2745   Accuracy: 0.918\n"," Test   Epoch: 001 / 010   Loss:  0.1404   Accuracy: 0.956\n","Train   Epoch: 002 / 010   Loss:  0.1008   Accuracy: 0.969\n"," Test   Epoch: 002 / 010   Loss: 0.09733   Accuracy: 0.968\n","Train   Epoch: 003 / 010   Loss: 0.06668   Accuracy: 0.979\n"," Test   Epoch: 003 / 010   Loss: 0.07625   Accuracy: 0.976\n","Train   Epoch: 004 / 010   Loss: 0.04695   Accuracy: 0.985\n"," Test   Epoch: 004 / 010   Loss: 0.07324   Accuracy: 0.977\n","Train   Epoch: 005 / 010   Loss: 0.03476   Accuracy: 0.989\n"," Test   Epoch: 005 / 010   Loss: 0.07059   Accuracy: 0.978\n","Train   Epoch: 006 / 010   Loss: 0.02579   Accuracy: 0.992\n"," Test   Epoch: 006 / 010   Loss: 0.06584   Accuracy: 0.980\n","Train   Epoch: 007 / 010   Loss: 0.01852   Accuracy: 0.994\n"," Test   Epoch: 007 / 010   Loss: 0.06502   Accuracy: 0.980\n","Train   Epoch: 008 / 010   Loss: 0.01293   Accuracy: 0.996\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bxim_oNddkqE"},"source":["# Submit Your Solution"]},{"cell_type":"code","metadata":{"id":"5OjcF__gXmth","cellView":"form"},"source":["#@title # Create and Download Your Solution\n","\n","import os\n","import re\n","import zipfile\n","from google.colab import files\n","\n","def create_zip(files, hw, name):\n","  zip_path = f'{hw}-{name}.zip'\n","  with zipfile.ZipFile(zip_path, 'w') as f:\n","    for fname in files:\n","      if not os.path.isfile(fname):\n","        raise FileNotFoundError(f\"Couldn't find file: '{fname}' in the homework directory\")\n","      f.write(fname, fname)\n","  return zip_path\n","\n","# export notebook as html\n","!jupyter nbconvert --to html hw2.ipynb\n","\n","##@markdown Please upload your typed solution (`.pdf` file) to the homework directory, and use the name `hw2-sol.pdf`.\n","\n","student_name = \"Itai Antebi\"  #@param{type:\"string\"}\n","assignment_name = 'hw2'\n","assignment_sol_files = ['hw2.ipynb', 'hw2.html', 'autograd.py', 'functional.py', 'nn.py', 'optim.py',\n","                        'models.py', 'models_torch.py', 'train.py', 'train_torch.py']\n","zip_name = re.sub('[_ ]+', '_', re.sub(r'[^a-zA-Z_ ]+', '', student_name.lower()))\n","\n","# create zip with your solution\n","zip_path = create_zip(assignment_sol_files, assignment_name, zip_name)\n","\n","# download the zip\n","files.download(zip_path)\n","\n","#@markdown Enter your name in `student_name` and run this cell to create and download a `.zip` file with your solution.\n","\n","#@markdown You should submit your solution via the Dropbox link given in Piazza.\n","\n","#@markdown **Note:** If you run this cell multiple times, you may be prompted by the browser to allow this page to download multiple files."],"execution_count":null,"outputs":[]}]}